import os
import uuid
import json
import pickle
from pathlib import Path
from datetime import datetime
from typing import Any, Dict
from pydantic import BaseModel
from .pdf_extraction import extract_pdf_image

from langchain.retrievers.multi_vector import MultiVectorRetriever
from langchain.storage import InMemoryStore
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from models import init_embeddings, init_chat_model, get_text_summary_chain, process_image_with_llava

class Element(BaseModel):
    type: str
    text: Any
    
class DiabetesKnowledgeBase:
    def __init__(self):
    
        # Initialize the knowledge base with the path to PDF files
        self.data_dir = "./data"
        self.image_dir = "./figures/"
        Path(self.image_dir).mkdir(parents=True, exist_ok=True)
        Path(self.data_dir).mkdir(parents=True, exist_ok=True)

        # Create a directory to store processed data
        self.processed_dir = os.path.join(self.data_dir, "processed")
        Path(self.processed_dir).mkdir(parents=True, exist_ok=True)

        # Create a file to track processed PDFs and their timestamps
        self.processed_log_path = os.path.join(self.processed_dir, "processed_files.json")
        self.processed_files = self._load_processed_files()

        # Create a path for the persisted vector store
        self.vectorstore_path = os.path.join(self.processed_dir, "chroma_db")
        self.docstore_path = os.path.join(self.processed_dir, "docstore.pkl")

        # Initialize components
        self.embeddings = init_embeddings()
        self.chat_model = init_chat_model()
        self.summary_chain = get_text_summary_chain()
        
        # Initialze the retriever system
        self.vectorstore = Chroma(
            collection_name="diabetes_kb",
            embedding_function=self.embeddings,
            persist_directory=self.vectorstore_path
        )

        # Load docstore if it exists, otherwise create new one
        if os.path.exists(self.docstore_path):
            with open(self.docstore_path, 'rb') as f:
                self.store = pickle.load(f)
            
            # Get document count using the yield_keys method
            try:
                doc_count = sum(1 for _ in self.store.yield_keys())
                print(f"Loaded existing docstore with {doc_count} documents.")
            except Exception as e:
                print(f"Loaded existing docstore. Unable to count documents: {str(e)}")
        else:
            self.store = InMemoryStore()
            print("Created new docstore!")
        
       
        self.id_key = "doc_id"
        
        self.retriever = MultiVectorRetriever(
            vectorstore=self.vectorstore,
            docstore=self.store,
            id_key=self.id_key
        )
        
        # Initialize RAG chain
        template = """You are a medical assistant specialized in diabetes. 
            Answer the question based only on the following context, which includes text, tables, and image descriptions:
            
            {context}
            
            Question: {question}
            
            Provide an accurate, helpful, and clear answer. If the information to answer the question is not contained in the context, 
            state that you don't have enough information rather than making up an answer.
            """
            
        self.prompt = ChatPromptTemplate.from_template(template)
        
        self.chain = (
            {"context": self.retriever, "question": RunnablePassthrough()} | self.prompt | self.chat_model | StrOutputParser()
        )
    

    # Load the log of processed files
    def _load_processed_files(self) -> Dict[str, str]:
        if os.path.exists(self.processed_log_path):
            with open(self.processed_log_path, 'r') as f:
                return json.load(f)
            
        return {}
    
    # Save the log of processed files
    def _save_processed_files(self):
        with open(self.processed_log_path, 'w') as f:
            json.dump(self.processed_files, f)

    # Save the docstore to disk
    def _persist_docstore(self):

        with open(self.docstore_path, 'wb') as f:
            pickle.dump(self.store, f)
        
        # Get document count using the yield_keys method
        try:
            doc_count = sum(1 for _ in self.store.yield_keys())
            print(f"Persisted docstore with {doc_count} documents.")
        except Exception as e:
            print(f"Persisted docstore. Unable to count documents: {str(e)}")

    # Process a PDF file, extract text, tables, and images  
    def process_pdf(self, pdf_path: str) -> None:
        filename = os.path.basename(pdf_path)
        file_mtime = str(os.path.getmtime(pdf_path))

        if filename in self.processed_files and self.processed_files[filename] == file_mtime:
            print(f"Skipping {filename} - already processed!")
            return 

        # Process text elements
        text_elements = [e for e in categorized_elements if e.type == "text" and e.text != ""]
        texts = [i.text for i in text_elements]
        print(f"Summarizing {len(texts)} text chunks...")
        text_summaries = self.summary_chain.batch(texts, {"max_concurrent": 5})

        # Save the interim results to prevent data loss in case of interruption
        interim_results_dir = os.path.join(self.processed_dir, f"{filename}_interim")
        Path(interim_results_dir).mkdir(parents=True, exist_ok=True)

        with open(os.path.join(interim_results_dir, "text_summaries.pkl"), 'wb') as f:
            pickle.dump(text_summaries, f)
        
        # Process table elements
        table_elements = [e for e in categorized_elements if e.type == "table"]
        tables = [i.text for i in table_elements]
        print(f"Summarizing {len(tables)} tables...")
        table_summaries = self.summary_chain.batch(tables, {"max_concurrent": 5})

        with open(os.path.join(interim_results_dir, "table_summaries.pkl"), 'wb') as f:
            pickle.dump(table_summaries, f)
        
        # Process image elements
        print("Processing extracted images ...")
        image_files = [f for f in os.listdir(self.image_dir) 
                       if f.lower().endswith((".png", ".jpg", ".jpeg")) and
                       os.path.getmtime(os.path.join(self.image_dir, f)) > os.path.getmtime(pdf_path) - 300] # Images extracted in the last 5 minutes
        
        # Print debugging information
        print(f"Found {len(image_files)} images to process")

        image_descriptions = {}
        for i, img_file in enumerate(image_files):
            img_path = os.path.join(self.image_dir, img_file)
            print(f"Processing image {i+1}/{len(image_files)}: {img_file}")
            description = process_image_with_llava(img_path)

            # Print the description to help debug
            print(f"Image description: {description[:100]}..." if len(description) > 100 else description)
    

            image_descriptions[img_file] = description

            # Save descriptions incrementally
            with open(os.path.join(interim_results_dir, "image_descriptions.json"), 'w') as f:
                json.dump(image_descriptions, f)
            
        # Add to retriever (only if there are items to add)
        if texts and text_summaries:
            self._add_to_retriever(texts, text_summaries, "text", filename)
            
        if tables and table_summaries:
            self._add_to_retriever(tables, table_summaries, "table", filename)
            
        if image_descriptions:
            self._add_to_retriever(list(image_descriptions.values()), list(image_descriptions.values()), "image", filename)
                
        # Mark as processed
        self.processed_files[filename] = file_mtime
        self._save_processed_files()
        
        # Persist the vectorstore
        self.vectorstore.persist()
        self._persist_docstore()

        print(f"Finished processing {filename}. Added {len(texts)} text chunks, {len(tables)} tables, and {len(image_descriptions)} images to knowledge base.")
        
        # Clean up interim files
        if os.path.exists(interim_results_dir):
            import shutil
            shutil.rmtree(interim_results_dir)

    # Add contents and their summaries to the retriever
    def _add_to_retriever(self, contents, summaries, content_type, source_file):
        # Skip if there's nothing to add
        if not contents or not summaries:
            print(f"No {content_type} content to add for {source_file} - skipping")
            return
        
        doc_ids = [str(uuid.uuid4()) for _ in contents]
        summary_docs = [
            Document(
                page_content=s, 
                metadata={
                    self.id_key: doc_ids[i], 
                    "type": content_type,
                    "source": source_file,
                    "date_added": datetime.now().isoformat()
                }
            )
            for i, s in enumerate(summaries)
        ]
        self.vectorstore.add_documents(summary_docs)
        self.store.mset(list(zip(doc_ids, contents)))
        print(f"Added {len(summary_docs)} {content_type} documents from {source_file}")
        
        
    # Process all PDFs in the data directory
    def process_all_pdfs(self):
        pdf_files = [f for f in os.listdir(self.data_dir) if f.lower().endswith('.pdf')]
        for pdf_file in pdf_files:
            pdf_path = os.path.join(self.data_dir, pdf_file)
            self.process_pdf(pdf_path)
            
    # Answer a  question using the RAG pipeline
    def answer_question(self, question: str) -> str:
        return self.chain.invoke(question)
    
    # Get a report of processed files
    def get_processed_files_status(self) -> str:
        if not self.processed_files:
            return "No files have been processed yet."
        
        report = "Processed files:\n"
        for filename, timestamp in self.processed_files.items():
            dt = datetime.fromtimestamp(float(timestamp))
            report += f"- {filename} (processed on {dt.strftime('%Y-%m-%d %H:%M:%S')})\n"
        
        return report
